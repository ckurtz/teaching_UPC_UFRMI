{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-Supervised Learning Demos.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckurtz/teaching_UPC_UFRMI/blob/main/selfsupervised_demos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBTL5OZDcX6"
      },
      "source": [
        "# References\n",
        "\n",
        "[Course Webpage](https://sites.google.com/view/berkeley-cs294-158-sp20/home)\n",
        "\n",
        "[1] Pathak, Deepak, et al. \"Context encoders: Feature learning by inpainting.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "\n",
        "[2] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).\n",
        "\n",
        "[3] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" arXiv preprint arXiv:2002.05709 (2020).\n",
        "\n",
        "[4] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.\n",
        "\n",
        "[5] Wang, Xiaolong, Allan Jabri, and Alexei A. Efros. \"Learning correspondence from the cycle-consistency of time.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n",
        "\n",
        "[6] Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\n",
        "\n",
        "[7] HÃ©naff, Olivier J., et al. \"Data-efficient image recognition with contrastive predictive coding.\" arXiv preprint arXiv:1905.09272 (2019).\n",
        "\n",
        "[8] Tian, Yonglong, Dilip Krishnan, and Phillip Isola. \"Contrastive multiview coding.\" arXiv preprint arXiv:1906.05849 (2019).\n",
        "\n",
        "[9] He, Kaiming, et al. \"Momentum contrast for unsupervised visual representation learning.\" arXiv preprint arXiv:1911.05722 (2019).\n",
        "\n",
        "[10] Doersch, Carl, Abhinav Gupta, and Alexei A. Efros. \"Unsupervised visual representation learning by context prediction.\" Proceedings of the IEEE International Conference on Computer Vision. 2015.\n",
        "\n",
        "[11] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhDFwV3dDsY5"
      },
      "source": [
        "# Getting Started\n",
        "Go to **Runtime -> Change runtime type** and make sure **Hardward accelerator** is set to **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR9QSsyEvY_U"
      },
      "source": [
        "!if [ -d cs294-158-ssl ]; then rm -Rf cs294-158-ssl; fi\n",
        "!git clone https://github.com/ckurtz/cs294-158-ssl\n",
        "!pip install cs294-158-ssl/\n",
        "\n",
        "import os\n",
        "os.chdir('cs294-158-ssl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrZxut8cQ_ps"
      },
      "source": [
        "Run the cells below to download the necessary pretrained models. It should take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPX6WS5QWSH"
      },
      "source": [
        "!wget https://camille-kurtz.com/teaching/data.zip\n",
        "!unzip -qq data.zip\n",
        "!rm data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRxGTJeSTSYy"
      },
      "source": [
        "!wget https://camille-kurtz.com/teaching/results.zip\n",
        "!unzip -qq results.zip\n",
        "!rm results.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZjp6-fFVEJ"
      },
      "source": [
        "The models and demos shown were pre-trained. The code used for all the demos can be found in the github repo [here](https://github.com/wilson1yan/cs294-158-ssl). Follow the README to train models on CIFAR10 or ImageNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8BTJ0Mt5oVy"
      },
      "source": [
        "# Self-Supervised Learning Tasks\n",
        "Self-supervised learning is a rapidly growing field, its success largely accelerated by growing compute and the vast amount of unlabeled data available for training. The hope is that by pretraining on specially designed self-supervised tasks, the models would be able to learn semantically meaningful representations to be used for downstream tasks. In the following demos, we will look at a few examples of these self-supervised tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2knEnPv38kOW"
      },
      "source": [
        "from deepul_helper.demos import load_model_and_data, evaluate_accuracy, display_nearest_neighbors, show_context_encoder_inpainting\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8KalQiXyJE-"
      },
      "source": [
        "## Demo 1: Context Encoder [[1]](https://arxiv.org/abs/1604.07379)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyHjNsSDzPY"
      },
      "source": [
        "The context encoder structures its self-supervised learning task by inpainting masked images. For example, the figure below shows different masking shapes, such as center masking, random block masking, and segmentation masking. Note that segmentation masking (c) is not purely self-supervised since we would need to train a image segmentation model which requires labels. However, the other two masking schemes (a) and (b) and purely self-supervised.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fhzkULYTtyMGUUF2n9dlPayJSdcY5pRv)\n",
        "\n",
        "More formally, the context encoder optimizes the following reconstruction loss:\n",
        "$$\\mathcal{L}_{rec} = \\left\\Vert \\hat{M} \\odot (x - F((1 - \\hat{M})\\odot x)) \\right\\Vert^2_2$$\n",
        "where $\\hat{M}$ is the masked region, $x$ is the image, and $F$ is the context encoder that tries to reconstruct the masked portion. In addition to the reconstruction loss, the paper introduces an adversarial loss that encourages more realistic inpaintings.\n",
        "$$L_{adv} = \\max_D \\mathbb{E}_{x\\in \\chi} [\\log(D(x)) + \\log(1 - D(F((1-\\hat{M})\\odot x)))]$$\n",
        "However, this demo does not use the adversarial portion of the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li_KasuCjQhp"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC-sluHYjSVB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ContextEncoder(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, dataset, n_classes):\n",
        "        super().__init__()\n",
        "        input_channels = 3\n",
        "\n",
        "        self.latent_dim = 4000\n",
        "\n",
        "        # Encodes the masked image\n",
        "        self.encoder = nn.Sequential(\n",
        "            # 128 x 128 Input\n",
        "            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1), # 64 x 64\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 64, 4, stride=2, padding=1), # 32 x 32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 16 x 16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 8 x 8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1), # 4 x 4\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, self.latent_dim, 4) # 1 x 1\n",
        "        )\n",
        "\n",
        "        # Only reconstructs the masked part of the image and not the whole image\n",
        "        self.decoder = nn.Sequential(\n",
        "           nn.BatchNorm2d(self.latent_dim),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(self.latent_dim, 512, 4, stride=1, padding=0), # 4 x 4\n",
        "           nn.BatchNorm2d(512),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1), # 8 x 8\n",
        "           nn.BatchNorm2d(256),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # 16 x 16\n",
        "           nn.BatchNorm2d(128),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), # 32 x 32\n",
        "           nn.BatchNorm2d(64),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.ConvTranspose2d(64, input_channels, 4, stride=2, padding=1), # 64 x 64\n",
        "           nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.BatchNorm1d(self.latent_dim, affine=False),\n",
        "            nn.Linear(self.latent_dim, self.n_classes)\n",
        "        )\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Extract a 64 x 64 center from 128 x 128 image\n",
        "        images_center = images[:, :, 32:32+64, 32:32+64].clone()\n",
        "        images_masked = images.clone()\n",
        "        # Mask out a 64 x 64 center with slight overlap\n",
        "        images_masked[:, 0, 32+4:32+64-4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64-4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64-4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "\n",
        "        z = self.encoder(images_masked)\n",
        "        center_recon = self.decoder(z)\n",
        "\n",
        "        return dict(Loss=F.mse_loss(center_recon, images_center)), torch.flatten(z, 1)\n",
        "\n",
        "    def encode(self, images):\n",
        "        images_masked = images\n",
        "        images_masked[:, 0, 32+4:32+64-4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64-4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64-4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "        return self.encoder(images_masked)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        images_center = images[:, :, 32:32+64, 32:32+64].clone()\n",
        "        images_masked = images.clone()\n",
        "        images_masked[:, 0, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 117.0/255.0 - 1.0\n",
        "        images_masked[:, 1, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 104.0/255.0 - 1.0\n",
        "        images_masked[:, 2, 32+4:32+64 - 4, 32+4:32+64-4] = 2 * 123.0/255.0 - 1.0\n",
        "\n",
        "        z = self.encoder(images_masked)\n",
        "        center_recon = self.decoder(z)\n",
        "\n",
        "        images_recon = images_masked.clone()\n",
        "        images_recon[:, :, 32:32+64, 32:32+64] = center_recon\n",
        "        return images_masked, images_recon\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVq8IbRwd9L"
      },
      "source": [
        "### Inpainting Examples\n",
        "For each pair of images, the left image is the masked input and the right the inpainted reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmsjo0U5EQjW"
      },
      "source": [
        "show_context_encoder_inpainting()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owIoT07hwl7b"
      },
      "source": [
        "### Linear Classification\n",
        "By design the model architecture is an encoder -> decoder module. We can use the bottleneck layer as our learned representation. Below, we show linear classification accuracy results on CIFAR10 using the learned representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW33IV4fw88x"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('context_encoder')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiLZdyXLw9Qo"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAxqYGEbxI8H"
      },
      "source": [
        "display_nearest_neighbors('context_encoder', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoTHjrBNzRyL"
      },
      "source": [
        "## Demo 2: Rotation Prediction [[2]](https://arxiv.org/abs/1803.07728)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cK8-rW0EQ8d"
      },
      "source": [
        "In this paper, the authors show that accurately predicting the degrees of rotation in images is a self-supervised learning task that learns good representations for downstream tasks.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
        "\n",
        "More specifically, the authors showed that training a common CNN architecture (AlexNet, ResNet) on the rotation task learns semantically interpretable convolutional masks similar to those learned in supervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBi6z7ajin2"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shrEf5ZqjkCn"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class RotationPrediction(nn.Module):\n",
        "    metrics = ['Loss', 'Acc1']\n",
        "    metrics_fmt = [':.4e', ':6.2f']\n",
        "\n",
        "    def __init__(self, dataset, n_classes):\n",
        "        super().__init__()\n",
        "        if dataset == 'cifar10':\n",
        "            self.model = NetworkInNetwork()\n",
        "            self.latent_dim = 192 * 8 * 8\n",
        "            self.feat_layer = 'conv2'\n",
        "        elif 'imagenet' in dataset:\n",
        "            self.model = AlexNet()\n",
        "            self.latent_dim = 256 * 13 * 13\n",
        "            self.feat_layer = 'conv5'\n",
        "        else:\n",
        "            raise Exception('Unsupported dataset:', dataset)\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        if self.dataset == 'cifar10':\n",
        "            classifier = nn.Sequential(\n",
        "                Flatten(),\n",
        "                nn.BatchNorm1d(self.latent_dim, affine=False),\n",
        "                nn.Linear(self.latent_dim, self.n_classes)\n",
        "            )\n",
        "        elif 'imagenet' in self.dataset:\n",
        "            classifier = nn.Sequential(\n",
        "                nn.AdaptiveMaxPool2d((6, 6)),\n",
        "                nn.BatchNorm2d(256, affine=False),\n",
        "                Flatten(),\n",
        "                nn.Linear(256 * 6 * 6, self.n_classes)\n",
        "            )\n",
        "        else:\n",
        "            raise Exception('Unsupported dataset:', dataset)\n",
        "        return classifier\n",
        "\n",
        "    def forward(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        images, targets = self._preprocess(images)\n",
        "        targets = targets.to(images.get_device())\n",
        "\n",
        "        logits, zs = self.model(images, out_feat_keys=('classifier', self.feat_layer))\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        correct = pred.eq(targets).float().sum()\n",
        "        acc = correct / targets.shape[0] * 100.\n",
        "\n",
        "        return dict(Loss=loss, Acc1=acc), zs[:batch_size]\n",
        "\n",
        "    def encode(self, images):\n",
        "        zs = self.model(images, out_feat_keys=(self.feat_layer,))\n",
        "        return zs\n",
        "\n",
        "    def _preprocess(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        images_90 = torch.flip(images.transpose(2, 3), (2,))\n",
        "        images_180 = torch.flip(images, (2, 3))\n",
        "        images_270 = torch.flip(images, (2,)).transpose(2, 3)\n",
        "        images_batch = torch.cat((images, images_90, images_180, images_270), dim=0)\n",
        "        targets = torch.arange(4).long().repeat(batch_size)\n",
        "        targets = targets.view(batch_size, 4).transpose(0, 1)\n",
        "        targets = targets.contiguous().view(-1)\n",
        "        return images_batch, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COZCFN75z4x6"
      },
      "source": [
        "### Linear Classification\n",
        "We can use the feature maps in the later convolutional layers of the pretrained model as our learned representation for linear classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhE1_yBqETzw"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('rotation')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DiA9kai0ncr"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sePwukfM0sef"
      },
      "source": [
        "display_nearest_neighbors('rotation', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB_cqJagEXbw"
      },
      "source": [
        "## Demo 3: SimCLR [[3]](https://arxiv.org/abs/2002.05709)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuLqNpTq1gkT"
      },
      "source": [
        "SimCLR is a contrastive learning framework to learn strong visual representations. An image $x$ is processed using data augmentation to produce two variants $x_i$ and $x_j$ which are both fed into encoder $f$ (a CNN) and projection head $g$ (a small MLP). The models optimize a contrastive loss to maximally align projected latents $z_i, z_j$. We consider $x_i, x_j$ as a positive pair, and any other $x_i, x_k$ pairs (i.e. different images in the same batch) are negative pairs. A visual diagram of the training procedure is shown below (from the paper).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1XW1uIkUTMSa0DZncivSYXzM5gA5FIhF6)\n",
        "\n",
        "More formally, the loss between positive example $z_i, z_j$ is:\n",
        "$$\\ell_{i,j} = -\\log{\\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N}\\mathbb{1}_{[k\\neq i]}\\exp(\\text{sim}(z_i, z_k)/\\tau)}}$$\n",
        "where $\\text{sim}(z_i, z_j) = z_i^Tz_j / (\\left\\Vert z_i \\right\\Vert \\left\\Vert z_j \\right\\Vert)$. The loss function can also be interpreted as a standard cross entropy loss to classify positive samples where logits are constructed using a given similarity function.\n",
        "\n",
        "Note: A common idea in contrastive learning methods is that a larger batch means more negative samples. Therefore, these methods usually benefit the most from large-batch learning compared to other self-supervised learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1iYSBMgjrVy"
      },
      "source": [
        "### Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExIYNEd3js5v"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from deepul_helper.resnet import resnet_v1\n",
        "from deepul_helper.batch_norm import SyncBatchNorm, BatchNorm1d\n",
        "\n",
        "# Some code adapted from https://github.com/sthalles/SimCLR\n",
        "class SimCLR(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, dataset, n_classes, dist=None):\n",
        "        super().__init__()\n",
        "        self.temperature = 0.5\n",
        "        self.projection_dim = 128\n",
        "\n",
        "        if dataset == 'cifar10':\n",
        "            resnet = resnet_v1((3, 32, 32), 50, 1, cifar_stem=True)\n",
        "            resnet = SyncBatchNorm.convert_sync_batchnorm(resnet)\n",
        "            self.resnet = resnet\n",
        "            self.latent_dim = 2048\n",
        "        elif 'imagenet' in dataset:\n",
        "            resnet = resnet_v1((3, 128, 128), 50, 1, cifar_stem=False)\n",
        "            if dist is not None:\n",
        "                resnet = nn.SyncBatchNorm.convert_sync_batchnorm(resnet)\n",
        "            self.resnet = resnet\n",
        "            self.latent_dim = 2048\n",
        "\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim, self.projection_dim, bias=False),\n",
        "            BatchNorm1d(self.projection_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(self.projection_dim, self.projection_dim, bias=False),\n",
        "            BatchNorm1d(self.projection_dim, center=False)\n",
        "        )\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.n_classes = n_classes\n",
        "        self.dist = dist\n",
        "\n",
        "    def construct_classifier(self):\n",
        "        return nn.Sequential(nn.Linear(self.latent_dim, self.n_classes))\n",
        "\n",
        "    def forward(self, images):\n",
        "        n = images[0].shape[0]\n",
        "        xi, xj = images\n",
        "        hi, hj = self.encode(xi), self.encode(xj) # (N, latent_dim)\n",
        "        zi, zj = self.proj(hi), self.proj(hj) # (N, projection_dim)\n",
        "        zi, zj = F.normalize(zi), F.normalize(zj)\n",
        "\n",
        "        # Each training example has 2N - 2 negative samples\n",
        "        # 2N total samples, but exclude the current and positive sample\n",
        "\n",
        "        if self.dist is None:\n",
        "            zis = [zi]\n",
        "            zjs = [zj]\n",
        "        else:\n",
        "            zis = [torch.zeros_like(zi) for _ in range(self.dist.get_world_size())]\n",
        "            zjs = [torch.zeros_like(zj) for _ in range(self.dist.get_world_size())]\n",
        "\n",
        "            self.dist.all_gather(zis, zi)\n",
        "            self.dist.all_gather(zjs, zj)\n",
        "\n",
        "        z1 = torch.cat((zi, zj), dim=0) # (2N, projection_dim)\n",
        "        z2 = torch.cat(zis + zjs, dim=0) # (2N * n_gpus, projection_dim)\n",
        "\n",
        "        sim_matrix = torch.mm(z1, z2.t()) # (2N, 2N * n_gpus)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "        # Mask out same-sample terms\n",
        "        n_gpus = 1 if self.dist is None else self.dist.get_world_size()\n",
        "        rank = 0 if self.dist is None else self.dist.get_rank()\n",
        "        sim_matrix[torch.arange(n), torch.arange(rank*n, (rank+1)*n)]  = -float('inf')\n",
        "        sim_matrix[torch.arange(n, 2*n), torch.arange((n_gpus+rank)*n, (n_gpus+rank+1)*n)] = -float('inf')\n",
        "\n",
        "        targets = torch.cat((torch.arange((n_gpus+rank)*n, (n_gpus+rank+1)*n),\n",
        "                             torch.arange(rank*n, (rank+1)*n)), dim=0)\n",
        "        targets = targets.to(sim_matrix.get_device()).long()\n",
        "\n",
        "        loss = F.cross_entropy(sim_matrix, targets, reduction='sum')\n",
        "        loss = loss / n\n",
        "        return dict(Loss=loss), hi\n",
        "\n",
        "    def encode(self, images):\n",
        "        return self.resnet(images[0])\n",
        "\n",
        "    def get_features(self, images):\n",
        "        return self.resnet.get_features(images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6HCMyM24bFQ"
      },
      "source": [
        "### Linear Classification\n",
        "We can use the encoded vector $h_i$ as our latent representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up_uOaJG4W2N"
      },
      "source": [
        "model, linear_classifier, train_loader, test_loader = load_model_and_data('simclr')\n",
        "evaluate_accuracy(model, linear_classifier, train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mnGhiXc5EuW"
      },
      "source": [
        "### Nearest Neighbors\n",
        "Another way to evaluate our learned representation is to look at nearest neighbors to random encoded images in latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQvrC7fQEZgC"
      },
      "source": [
        "display_nearest_neighbors('simclr', model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqGOS_456L8G"
      },
      "source": [
        "## Other Tasks\n",
        "In addition to the above tasks, prior work has also investigated other self-superivsed tasks such as solving jigsaw puzzles [[4]](https://arxiv.org/abs/1603.09246), cycle-consistency [[5]](https://arxiv.org/abs/1903.07593), contrastive learning [[6]](https://arxiv.org/abs/1807.03748)[[7]](https://arxiv.org/abs/1905.09272)[[8]](https://arxiv.org/abs/1906.05849)[[9]](https://arxiv.org/abs/1911.05722), and patch prediction [[10]](https://arxiv.org/abs/1505.05192). See [here](https://github.com/jason718/awesome-self-supervised-learning) for a great resource on more self-supervised learning papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UznQoUaiEaCX"
      },
      "source": [
        "# Demo 4: Using Representations for Downstream Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O3ICsfdkGoF"
      },
      "source": [
        "After pretraining a model on a self-supervised learning task, we can use it for other downstream tasks. In this demo, we use the pre-trained ResNet50 backbone from training SimCLR on a subset of ImageNet to learn a semantic segmentation model on Pascal VOC 2012. We use a simple U-Net [[11]](https://arxiv.org/abs/1505.04597) architecture with skip connections across feature maps between the SimCLR encoder and learned upsampling decoder. We do not fine-tune the SimCLR ResNet50 backbone, and only optimize the upsampling portion.\n",
        "\n",
        "![](https://drive.google.com/uc?id=19dxxcwof0IA0jyv0VCl4rnZZf3ajA22s)\n",
        "\n",
        "The training script can be found in `train_segmentation.py` [here](https://github.com/wilson1yan/cs294-158-ssl/blob/master/train_segmentation.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_z7-YFoYCwR"
      },
      "source": [
        "## Example Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJuWNENYGT9"
      },
      "source": [
        "# Code adapted from https://github.com/qubvel/segmentation_models.pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from deepul_helper.resnet import NormReLU\n",
        "\n",
        "class SegmentationModel(nn.Module):\n",
        "    metrics = ['Loss']\n",
        "    metrics_fmt = [':.4e']\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        decoder_channels = (512, 256, 128, 64, 32)\n",
        "        encoder_channels = (2048, 1024, 512, 256, 64) # Starting from head (resnet 50)\n",
        "\n",
        "        # Construct decoder blocks\n",
        "        in_channels = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
        "        skip_channels = list(encoder_channels[1:]) + [0]\n",
        "        out_channels = decoder_channels\n",
        "        blocks = [\n",
        "            DecoderBlock(in_ch, skip_ch, out_ch)\n",
        "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
        "        ]\n",
        "        self.dec_blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # Segmentation head for output prediction\n",
        "        self.seg_head = nn.Conv2d(decoder_channels[-1], n_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, features, targets):\n",
        "        features = features[1:] # remove first skip with same spatial resolution\n",
        "        features = features[::-1] # reverse channels to start from head of encoder\n",
        "\n",
        "        skips = features[1:]\n",
        "        x = features[0]\n",
        "        for i, decoder_block in enumerate(self.dec_blocks):\n",
        "            skip = skips[i] if i < len(skips) else None\n",
        "            x = decoder_block(x, skip)\n",
        "\n",
        "        logits = self.seg_head(x)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return dict(Loss=loss), logits\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            skip_channels,\n",
        "            out_channels,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels + skip_channels, out_channels,\n",
        "                      kernel_size=3, padding=1),\n",
        "            NormReLU((out_channels, None, None)), # only care about channel dim for BN\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            NormReLU((out_channels, None, None))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXcDxUwYYHgM"
      },
      "source": [
        "## Segmentation Results\n",
        "Below, we show a random subset of segmentations from the trained model. Every set of 3 images consists of the original image, the labeled segmentation, and the predicted segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYUmet3REeIa"
      },
      "source": [
        "from deepul_helper.demos import show_segmentation\n",
        "show_segmentation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI94rsx_Efro"
      },
      "source": [
        "# Demo 5: Avoiding Trivial Representations in Self-Supervised Tasks\n",
        "\n",
        "When designing a self-supervised learning task, it is important to make sure that no trivial solutions exists. In general, a learned solution is trivial if the model is able to successfully complete its task by taking advantage of low-level features. As a result, it doesn't learn a good representation so downstream performance is bad.\n",
        "\n",
        "For example, in the jigsaw [[4]](https://arxiv.org/abs/1603.09246) task, a model can \"cheat\" by just looking at the boundary textures of the jigsaw pieces, or following and matching straight lines across different pieces. These issues can generally be fixed by ranndom cropping, shifting, and spacially jittering.\n",
        "\n",
        "We look at two other less obvious aspects of images that may reduce performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmE4AFUGgi3"
      },
      "source": [
        "## Chromatic Aberration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKZH3vOTRtM"
      },
      "source": [
        "Chromatic aberration occurs when the different focal lengths of light results in the light not meeting all at the same point.\n",
        "![from wikipedia](https://drive.google.com/uc?id=1PYGoQWnH0aAeiE_8t4ef5WDcq1UIQQ5t)\n",
        "\n",
        "A example of very apparent chromatic aberration is shown below, where the green and magenta colors are clearly offset with each other:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1M1B6kV6ddBwyJse3FQT8_XBTeqs5s5WL)\n",
        "\n",
        "Chromatic aberration generally becomes a problem in patch-based self-supervised learning tasks that design, such as solving jigsaw puzzles, or predicintg the correct location of a patch in an image In this case, the model can take advantage of the low-level chromatic aberration features to get a strong idea of where the patch is located without understanding the actual context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lcNZIa9VHw-"
      },
      "source": [
        "Below is a quick demo of chromatic aberration in more realistic images, and possible fixes. Note that in general, chromatic aberration is fairly hard to spot with the naked eye, but deep learning models are still able to use it to their advantage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4A0BVarEp4C"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You can see some chromatic aberration in the purple fringes around the branches\n",
        "\n",
        "image = Image.open('sample_images/chrom_ab_demo.png')\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViS8riD8WEyr"
      },
      "source": [
        "Chromatic aberration is generally fixed through conversion to grayscale, or color dropping. Color dropping works by dropping 2 of the color channels and replacing them with random noise uniform or Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXyV8JP2WES8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Color Dropping\n",
        "# We will drop all channels except R\n",
        "image_cpy = image.copy()\n",
        "pixels = image_cpy.load()\n",
        "\n",
        "arr = np.array(image_cpy)\n",
        "std_R = np.std(arr[:, :, 0])\n",
        "mean_G, mean_B = np.mean(arr[:, :, 1]), np.mean(arr[:, :, 2])\n",
        "\n",
        "for i in range(image.size[0]):\n",
        "  for j in range(image.size[1]):\n",
        "    p = pixels[i, j] # (R, G, B, A)\n",
        "    R, A = p[0], p[3]\n",
        "    G = int(np.random.randn() * std_R + mean_G)\n",
        "    B = int(np.random.randn() * std_R + mean_B)\n",
        "    pixels[i, j] = (R, G, p[2], p[3])\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Color Dropping')\n",
        "plt.axis('off')\n",
        "plt.imshow(image_cpy)\n",
        "plt.show()\n",
        "\n",
        "# Grayscale\n",
        "image_cpy2 = image.copy()\n",
        "pixels2 = image_cpy2.load()\n",
        "\n",
        "for i in range(image.size[0]):\n",
        "  for j in range(image.size[1]):\n",
        "    p = pixels[i, j]\n",
        "    G = int(0.3 * p[0] + 0.59 * p[1] + 0.11 * p[2])\n",
        "    pixels2[i, j] = (G, G, G, 255)\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Grayscale')\n",
        "plt.axis('off')\n",
        "plt.imshow(image_cpy2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHDJB6XdH9bq"
      },
      "source": [
        "## Color Intensity Histograms\n",
        "\n",
        "In the SimCLR paper, the authors show that the histogram of color intensities of different patches within the same image have very similar histograms, which may degrade training by encouraging models to look at low-level (pixel intensity) features to solve self-supervised tasks that involve matching positive patches of the same image.\n",
        "\n",
        "Below, we run a similar demo to what was demonstrated in the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPiDcnVgICJV"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = Image.open('sample_images/n01537544_19414.JPEG')\n",
        "plt.figure()\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47ATyUihBv6"
      },
      "source": [
        "import numpy as np\n",
        "arr = np.array(image)\n",
        "H, W, _ = arr.shape\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  r = np.random.randint(0, H - 128)\n",
        "  c = np.random.randint(0, W - 128)\n",
        "  patch = arr[r:r+128, c:c+128]\n",
        "\n",
        "  axs[i].set_title(f'Patch {i}')\n",
        "  axs[i].hist(patch.reshape(-1), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5F_c4JDi6PX"
      },
      "source": [
        "Now we apply color jittering to mitigate this effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQicXcDwhNif"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
        "jitter_img = color_jitter(image)\n",
        "\n",
        "image = Image.open('sample_images/n01537544_19414.JPEG')\n",
        "plt.figure()\n",
        "plt.title('Color Jittered Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(jitter_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOPbrPuXiqjc"
      },
      "source": [
        "arr = np.array(image)\n",
        "H, W, _ = arr.shape\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i in range(4):\n",
        "  r = np.random.randint(0, H - 128)\n",
        "  c = np.random.randint(0, W - 128)\n",
        "  patch = arr[r:r+128, c:c+128]\n",
        "\n",
        "  axs[i].set_title(f'Patch {i}')\n",
        "  axs[i].hist(patch.reshape(-1), bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnlJmV51i1vJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}