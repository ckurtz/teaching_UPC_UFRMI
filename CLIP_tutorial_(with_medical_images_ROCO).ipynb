{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ckurtz/teaching_UPC_UFRMI/blob/main/CLIP_tutorial_(with_medical_images_ROCO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "Notebook created in 2024 with the help of Xiaoyang Wei, Phd student at LIPADE, Université Paris Cité\n",
    "\n",
    "# Interacting with CLIP\n",
    "\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Preparation for Colab\n",
    "\n",
    "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BpdJkdBssk9"
   },
   "outputs": [],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLFS29hnhlY4"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBRVTY9lbGm8"
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/16\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21slhZGCqANb"
   },
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
    "\n",
    "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6cpiIFHp9N6"
   },
   "outputs": [],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwSB5jZki3Cj"
   },
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGom156-i2kL"
   },
   "outputs": [],
   "source": [
    "clip.tokenize(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4W8ARJVqBJXs"
   },
   "source": [
    "# Setting up input images and texts\n",
    "\n",
    "We are going to feed 8 example natrural/medical images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
    "\n",
    "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecrPUADIPkQ3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# to mount google drive\n",
    "if os.getcwd()=='/content':\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        base_working_dir = '/content/drive/MyDrive'\n",
    "        drive.mount('/content/drive')\n",
    "    except:\n",
    "        base_working_dir = os.getcwd()\n",
    "        pass\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "print(base_working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tMc1AXzBlhzm"
   },
   "outputs": [],
   "source": [
    "# images to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\",\n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}\n",
    "\n",
    "med_descriptions = {\n",
    "    \"ROCO_00002\": \"CT scan in axial view showing obliteration of the left maxillary sinus\",\n",
    "    \"ROCO_00017\": \"Coronary angiogram demonstrating occluded arteries\",\n",
    "    \"ROCO_00028\": \"T2 weighted MRI image showing foraminal extensions of the cysts\",\n",
    "    \"ROCO_00033\": \"Lytic lesions (arrows) involving the humerus.\",\n",
    "    \"ROCO_00050\": \"Free air beneath the diaphragm at abdominal x-ray film\",\n",
    "    \"ROCO_00056\": \"View of giant cell tumor of thumb metacarpal preoperatively\",\n",
    "    \"ROCO_00060\": \"Transverse view of lung using CT.Leukemic infiltration is seen.\",\n",
    "    \"ROCO_00135\": \"CT scan showing multiple non-communicating cysts in liver and kidney\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSSrLY185jSf"
   },
   "outputs": [],
   "source": [
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "skimage.data.download_all(directory=None)\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "\n",
    "    plt.subplot(2, 4, len(images) + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(descriptions[name])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVKsji6WOIX"
   },
   "source": [
    "## Building features\n",
    "\n",
    "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HBgCanxi8JKw"
   },
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuxm2Gt4Wvzt"
   },
   "source": [
    "## Calculating cosine similarity\n",
    "\n",
    "We normalize the features and calculate the dot product of each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5zvMxh8cU6m"
   },
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "# plt.colorbar()\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features(natural)\", size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IirQtvIRLlTE"
   },
   "source": [
    "## Performance of CLIP in medical domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SC46CjfZL3_Z"
   },
   "outputs": [],
   "source": [
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "for filename in list(med_descriptions.keys()):\n",
    "    image = Image.open(os.path.join(\"/content/drive/My Drive/data/pretrain_data/roco/train/radiology/images\", filename+'.jpg'))\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(med_descriptions[filename])\n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "# plt.colorbar()\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features(medical)\", size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alePijoXy6AH"
   },
   "source": [
    "# Zero-Shot Image Classification\n",
    "\n",
    "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqu4GlfPfr-p"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "C4S__zCGy2MT"
   },
   "outputs": [],
   "source": [
    "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n",
    "text_tokens = clip.tokenize(text_descriptions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "c4z1fm9vCpSR"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6Ju_6IBE2Iz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.subplot(4, 4, 2 * i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(4, 4, 2 * i + 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs[i])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
    "    plt.xlabel(\"probability\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF392ouREqN9"
   },
   "source": [
    "# Fine-tuning CLIP on ROCO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cVuI-kQ3zllj"
   },
   "outputs": [],
   "source": [
    "# dataset location\n",
    "csv_file =\"/content/drive/My Drive/data/roco-list.csv\"\n",
    "root_dir = \"/content/drive/My Drive/data/pretrain_data/roco/train/radiology/images\"\n",
    "val_csv_file = \"/content/drive/My Drive/data/local-val-list.csv\"\n",
    "val_root_dir = \"/content/drive/My Drive/data/pretrain_data/roco/val/radiology/images\"\n",
    "\n",
    "# train options\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "image_size = 224\n",
    "start_epoch = 0\n",
    "epochs = 10\n",
    "visual_encoder = \"clip@ViT-B/16\"\n",
    "learning_rate = 3e-6\n",
    "model_path = \"/content/drive/My Drive/model_checkpoint_CLIP_ROCO\" # set to the directory containing `checkpoint_##.tar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkbpSrUdzmXq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import clip\n",
    "\n",
    "\n",
    "\n",
    "class CLRDataset(Dataset):\n",
    "    \"\"\"Contrastive Learning Representations Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, clip=False):\n",
    "        self.clr_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.clip = clip\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clr_frame)\n",
    "\n",
    "    def text_sampling(self, text):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.split(\".\")\n",
    "        if '' in text:\n",
    "            text.remove('')\n",
    "        text = random.choice(text)\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.clr_frame.iloc[idx, 0]\n",
    "                                )\n",
    "        if not os.path.exists(img_name):\n",
    "            img_name=img_name.replace('train','val')\n",
    "        image = Image.open(img_name)\n",
    "        if not self.clip:\n",
    "            image = image.convert('RGB')\n",
    "        text = self.clr_frame.iloc[idx, 1]\n",
    "        name = self.clr_frame.iloc[idx, 0]\n",
    "        sample = {'image': image, 'text': text,'name':name}\n",
    "        if self.clip:\n",
    "            sample = self.transform(sample['image']), sample['text'],sample['name'] #self.text_sampling(sample['text'])\n",
    "        elif self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_optimizer( model, lr=3e-6):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, epochs, eta_min=0, last_epoch=-1\n",
    "    )\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def train( train_loader, model, tokenizer, optimizer,truncation):\n",
    "    loss_epoch = 0\n",
    "    for step, (images, texts,name) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x_v = images.to('cuda')\n",
    "        x_u = tokenizer(list(texts), truncate=True).to('cuda')\n",
    "\n",
    "        v,u=model(x_v,x_u)\n",
    "\n",
    "        labels = torch.arange(batch_size, dtype=torch.long, device='cuda')\n",
    "\n",
    "        loss_img = torch.nn.CrossEntropyLoss()\n",
    "        loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        img_loss = loss_img(v,labels)\n",
    "        text_loss = loss_txt(u,labels)\n",
    "\n",
    "        loss = (img_loss + text_loss)/2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch\n",
    "\n",
    "\n",
    "\n",
    "def validate( val_loader, model, tokenizer, optimizer, truncation):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_epoch = 0\n",
    "        for step, (x_v, x_u) in enumerate(val_loader):\n",
    "            x_v = x_v.to('cuda')\n",
    "            x_u = tokenizer(list(x_u), truncate=True).to('cuda')\n",
    "\n",
    "            v,u=model(x_v,x_u)\n",
    "\n",
    "            labels = torch.arange(batch_size, dtype=torch.long, device='cuda')\n",
    "\n",
    "            loss_img = torch.nn.CrossEntropyLoss()\n",
    "            loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            img_loss = loss_img(v,labels)\n",
    "            text_loss = loss_txt(u,labels)\n",
    "\n",
    "            loss = (img_loss + text_loss)/2\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return loss_epoch\n",
    "\n",
    "\n",
    "def main(gpu):\n",
    "\n",
    "    is_master=True\n",
    "\n",
    "    # set GPU device\n",
    "    torch.cuda.set_device(gpu)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # initialize model\n",
    "    print(\"Initializing model... \", end=\"\", flush=True)\n",
    "    model, preprocess = clip.load(visual_encoder.split(\"@\")[-1], device='cuda', jit=False)\n",
    "    model.float()\n",
    "    tokenizer=clip.tokenize\n",
    "    train_fonction = train\n",
    "    validate_fonction = validate\n",
    "\n",
    "    # optimizer\n",
    "    optimizer, scheduler = load_optimizer(model, lr=float(learning_rate))\n",
    "\n",
    "    # initialize dataloader\n",
    "    transform = preprocess\n",
    "    print('Image_Transform',transform)\n",
    "    train_dataset = CLRDataset(csv_file=csv_file,\n",
    "                               root_dir=root_dir,\n",
    "                               transform=transform,\n",
    "                               clip = True\n",
    "                               )\n",
    "\n",
    "    val_dataset = CLRDataset(csv_file=val_csv_file,\n",
    "                             root_dir=val_root_dir,\n",
    "                             transform=transform,\n",
    "                             clip = True\n",
    "                             )\n",
    "\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        drop_last=True,\n",
    "        num_workers=8,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    print(\"[DONE]\\n\")\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print('Start Time =', datetime.datetime.now().strftime(\"%H:%M:%S\"), '\\n')\n",
    "\n",
    "    t0 = time.time()\n",
    "    global_step = 0\n",
    "    current_epoch = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        if epoch<2:\n",
    "            if train_sampler is not None:\n",
    "                train_sampler.set_epoch(epoch)\n",
    "\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            loss_epoch = train_fonction(train_loader, model, tokenizer, optimizer,True)\n",
    "\n",
    "        if is_master:\n",
    "            val_loss = validate_fonction( val_loader, model, tokenizer, optimizer, True)\n",
    "            if val_loss < best_val_loss:\n",
    "                torch.save(model.state_dict(), os.path.join(model_path, \"best_checkpoint.pth\"))\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "            epoch_counter = epoch - 0\n",
    "            elapsed = time.time() - t0\n",
    "            epoch_time = elapsed/(epoch_counter+1)\n",
    "            remaining = (epochs - (epoch_counter+1))*epoch_time\n",
    "            remaining = str(datetime.timedelta(seconds=round(remaining)))\n",
    "            elapsed = str(datetime.timedelta(seconds=round(elapsed)))\n",
    "            print(f'Epoch {epoch_counter+1}/{epochs} [{elapsed}<{remaining}, {round(epoch_time, 2)}s/epoch] {round((epoch_counter+1)/epochs*100, 1)}% loss: {loss_epoch / len(train_loader)}\\t val_loss: {val_loss / len(val_loader)} lr: {lr}')\n",
    "\n",
    "            current_epoch += 1\n",
    "\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "main(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asBuLIrqNfbA"
   },
   "source": [
    "## Performance of CLIP in medical domain(after fine-tuning on ROCO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sK5lZckENl71"
   },
   "outputs": [],
   "source": [
    "#load checkpoint\n",
    "model.load_state_dict(torch.load('/content/drive/My Drive/data/ViT-B-16_best_checkpoint_2.tar'))\n",
    "\n",
    "#compute txt-img similarity again\n",
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "for filename in list(med_descriptions.keys()):\n",
    "    image = Image.open(os.path.join(root_dir, filename+'.jpg'))\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(med_descriptions[filename])\n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "\n",
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features(medical)\", size=20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
